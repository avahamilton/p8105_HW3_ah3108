---
title: "p8105_hw3_ah3108"
author: "Ava Hamilton"
date: "10/8/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
#library(viridis)
library(p8105.datasets)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
options(
  ggplot2.continuous.colour = "rainbow",
  ggplot2.continuous.fill = "rainbow"
)
scale_colour_discrete = scale_color_hue()
scale_fill_discrete = scale_fill_hue()
theme_set(theme_minimal() + theme(legend.position = "bottom"))


```

# Problem 1
### Examining Instacart data

```{r}

data("instacart")

#The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):

instacart = 
  instacart %>% 
  janitor::clean_names() %>% 
  mutate(
    # aisle = as.factor(aisle),
    department = as.factor(department)
  )


orderinfo = 
  instacart %>% 
  group_by(order_id) %>% 
  summarize(num_in_order = max(add_to_cart_order, na.rm = TRUE),
            num_reorders = sum(reordered, na.rm = TRUE),
            ratio_reorder = num_reorders/num_in_order) %>% 
  ungroup() %>% 
  view()


```

The *instacart* data has `r ncol(instacart)` variables and `r nrow(instacart)` observations, where each observation is a product ordered on the online grocery service, Instacart. The data contains `r nrow(distinct(instacart, order_id))` orders from `r nrow(distinct(instacart, user_id))` shoppers. The median number of items ordered was `r median(pull(orderinfo, num_in_order))`, and ranged from `r min(pull(orderinfo, num_in_order))` to `r max(pull(orderinfo, num_in_order))` items with an average `r  round(100*(mean(pull(orderinfo, ratio_reorder))), 2)`% reordered items.

```{r}
# Part A - How many aisles are there, and which aisles are the most items ordered from?

items = 
  instacart %>% 
  add_count(aisle, sort = TRUE, name = "items_from_aisle")  %>% 
  distinct(aisle, items_from_aisle) %>% 
  arrange(desc(items_from_aisle))

```

There are `r nrow(distinct(instacart, aisle))` aisles, with the maximum number of items ordered from the `r items %>% slice(1) %>%   magrittr::extract2("aisle")` aisle (N = `r max(pull(items,items_from_aisle)) `) followed by `r items %>% slice(2) %>%   magrittr::extract2("aisle")`, `r items %>% slice(3) %>%   magrittr::extract2("aisle")`, `r items %>% slice(4) %>%   magrittr::extract2("aisle")`, and `r items %>% slice(5) %>%   magrittr::extract2("aisle")`.


```{r}
# Part B - plot of number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

items %>% 
  filter(items_from_aisle > 10000) %>%
  ggplot(aes(x = reorder(aisle, -items_from_aisle), y = items_from_aisle)) +
  geom_point() +
  scale_y_continuous(breaks = seq(10000, 150000, 10000)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Number of items ordered from each aisle",
       x = "Aisle",
       y = "Number of Items Ordered")

```


# AVA COMMENT

```{r}
# Part C 
#Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

top_products = instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  add_count(product_name, sort = TRUE, name = "items_in_aisle")  %>% 
  distinct(aisle, product_name, items_in_aisle) %>% 
  group_by(aisle) %>% 
  mutate(
    rank_product = rank(-items_in_aisle)
  ) %>% 
  filter(rank_product <= 3) %>% 
  dplyr::select(rank_product, everything()) %>% 
  kableExtra::kable()

top_products

# product_table = pivot_wider(top_products,
#                            names_from = aisle,
#                            values_from = c(product_name, items_in_aisle)
#) %>% 
#  kableExtra::kable() 

```

# AVA COMMENT


```{r}
# Part D:
#Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).


```


# AVA COMMENT


# Problem 2
### Examining BRFSS data

```{r}

data("BRFSS")  

brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(question = as.factor(question),
         response = as.factor(response),
         response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered = T),
         question = as.factor(question)
  ) %>% 
  rename(
    health_rating = response,
    percent = data_value,
    state = locationabbr,
    county = locationdesc,
  ) %>% 
  dplyr::select(-c("question", "topic", "class", "topic_id", "location_id", "question_id", "class_id", "data_source", "data_value_type", "data_value_unit", "data_value_footnote", "data_value_footnote_symbol"))


# In 2002, which states were observed at 7 or more locations? What about in 2010?


# Part A
loca_df = 
  brfss %>% 
  distinct(state, county, year) %>% 
  arrange(year, state, county) %>% 
  group_by(state, year) %>% 
  add_count(state, name = "num_locations") %>% 
  distinct(year, state, num_locations) %>% 
  filter(num_locations >= 7)



```

# AVA write inline comments for part A using data above


```{r}

# Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).


#Part B
brfss %>% 
  filter(health_rating == "Excellent") %>% 
  group_by(state, year) %>% 
  mutate(
    data_avg = mean(percent, na.rm = TRUE)
  ) %>% 
  distinct(state, year, data_avg) %>%
  ggplot(aes(x = year, y = data_avg, color = state)) +
  geom_line(aes(group = state)) +
  labs(title = "Average excellent rating across time, by state",
       x = "Year",
       y = "Across state average percent excellent rating")

```




```{r}

#Part C

# Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.




```





# Problem 3

```{r message = FALSE}
# read in data

# Part A - wrangle data
accel_df = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  rename(day_of_week = day) %>% 
  mutate(
    day_of_week = as.factor(day_of_week),
    day_of_week = ordered(day_of_week, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    weekend = ifelse(day_of_week %in% c("Saturday", "Sunday"), "weekend", "weekday"),
    weekend = as.factor(weekend)
  ) %>% 
  arrange(week, day_of_week) %>% 
  mutate(
    order_id = sequence(n())
  ) %>% 
  select(order_id, day_id, week, day_of_week, weekend, everything())


```

#AVA COMMENT

```{r}
# Part B - aggregate across activity

accel_df %>% 
  mutate(
    act_sum = select(., activity_1:activity_1440) 
    %>% apply(1, sum, na.rm = TRUE),
  ) %>% 
  select(act_sum, week, day_of_week) %>% 
  pivot_wider(
    names_from = day_of_week,
    values_from = act_sum
  ) %>% 
  kableExtra::kable(format.args = list(big.mark = ","), digits = 0) 



```
# AVA COMMENT

```{r}
# Part C - figure across day

long_accel = accel_df %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "min_of_day",
    names_prefix = "activity_",
    values_to = "activity_per_min"
  ) %>% 
  mutate(
    min_of_day = as.integer(min_of_day)
  )


long_accel %>% 
  #filter(week == 1) %>% 
  ggplot(aes(x = min_of_day, y = activity_per_min, color = day_of_week)) +
  geom_line(aes(group = day_id), alpha = 0.7) +
  scale_x_continuous(breaks = seq(0, 1400, 200)) +
  scale_color_hue("Day of the week")  +
  labs(title = "Daily Accelerometer Activity Over 5 Weeks for 63 Year-old Male",
       x = "Minute of the Day",
       y = "Accelerometer Activity Level")

```


Based on the figure above, in general, there is higher latenight activity on Fridays, some late afternoon activity on saturdays, and higher daytime activity on Sundays. There also seems to be some early morning activity Sunday mornings, which would most likely mean continuation of activity from Saturday night.
