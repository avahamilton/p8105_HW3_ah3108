---
title: "p8105_hw3_ah3108"
author: "Ava Hamilton"
date: "10/8/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(p8105.datasets)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))


```

# Problem 1
### Examining Instacart data

```{r}

#Part A

data("instacart")

instacart = 
  instacart %>% 
  janitor::clean_names() %>% 
  mutate(
    # aisle = as.factor(aisle),
    department = as.factor(department)
  )


orderinfo = 
  instacart %>% 
  group_by(order_id) %>% 
  summarize(num_in_order = max(add_to_cart_order, na.rm = TRUE),
            num_reorders = sum(reordered, na.rm = TRUE),
            ratio_reorder = num_reorders/num_in_order) %>% 
  ungroup() %>% 
  view()


```

The *instacart* data has `r ncol(instacart)` variables and `r nrow(instacart)` observations, where each observation is a product ordered on the online grocery service, Instacart. The data contains `r nrow(distinct(instacart, order_id))` orders from `r nrow(distinct(instacart, user_id))` shoppers. The median number of items ordered was `r median(pull(orderinfo, num_in_order))`, and ranged from `r min(pull(orderinfo, num_in_order))` to `r max(pull(orderinfo, num_in_order))` items with an average `r  round(100*(mean(pull(orderinfo, ratio_reorder))), 2)`% reordered items.

### Frequently ordered from aisles
```{r}
# Part A - How many aisles are there, and which aisles are the most items ordered from?

items = 
  instacart %>% 
  add_count(aisle, sort = TRUE, name = "items_from_aisle")  %>% 
  distinct(aisle, items_from_aisle) %>% 
  arrange(desc(items_from_aisle))

```

There are `r nrow(distinct(instacart, aisle))` aisles, with the maximum number of items ordered from the _`r items %>% slice(1) %>%   magrittr::extract2("aisle")`_ aisle (N = `r max(pull(items,items_from_aisle)) `) followed by _`r items %>% slice(2) %>%   magrittr::extract2("aisle")`_, _`r items %>% slice(3) %>%   magrittr::extract2("aisle")`_, _`r items %>% slice(4) %>%   magrittr::extract2("aisle")`_, and _`r items %>% slice(5) %>%   magrittr::extract2("aisle")`_.

```{r}
# Part B - plot of number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

items_plot =
  items %>%
  filter(items_from_aisle > 10000)

items_plot %>% 
  ggplot(aes(x = reorder(aisle, -items_from_aisle), y = items_from_aisle)) +
  geom_point() +
  scale_y_continuous(breaks = seq(10000, 150000, 10000)) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Number of items ordered from each aisle",
       x = "Aisle",
       y = "Number of Items Ordered")

```

Items from the top two aisles, _`r items_plot %>% slice(1) %>%   magrittr::extract2("aisle")`_ and _`r items_plot %>% slice(2) %>%   magrittr::extract2("aisle")`_ are ordered with approximately equal frequency(`r items_plot %>% slice(1) %>%   magrittr::extract2("items_from_aisle")` vs. `r items_plot %>% slice(2) %>%   magrittr::extract2("items_from_aisle")`), and almost twice as often as the products from _`r items_plot %>% slice(3) %>%   magrittr::extract2("aisle")`_, the third most frequently ordered from aisle (N = `r items_plot %>% slice(3) %>%   magrittr::extract2("items_from_aisle")`). Following the top three shopped aisles the spread between the rest of the aisles with more than 10,000 orders decreases, with a median number of orders from the `r pull(items_plot, aisle)[pull(items_plot,items_from_aisle) == median(pull(items_plot,items_from_aisle))]` aisle, where N = `r median(pull(items_plot, items_from_aisle))`.

### Frequent items in 3 aisles
```{r}
# Part C 
#Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

top_products = instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  add_count(product_name, sort = TRUE, name = "items_in_aisle")  %>% 
  distinct(aisle, product_name, items_in_aisle) %>% 
  group_by(aisle) %>% 
  mutate(
    rank_product = rank(-items_in_aisle)
  ) %>% 
  filter(rank_product <= 3) %>% 
  dplyr::select(rank_product, everything())

top_products %>% 
  kableExtra::kable()

```

The range of number of items ordered for the top three products in each of the aisles vary greatly. For _packaged vegetables fruits_ the variance between the number of items ordered in the top product is quite large with a mean(SD) of `r round(mean(pull(top_products %>% filter(aisle == "packaged vegetables fruits"), items_in_aisle)),2)`(`r round(sd(pull(top_products %>% filter(aisle == "packaged vegetables fruits"), items_in_aisle)),2)`), compared to that of _baking ingredients_, `r round(mean(pull(top_products %>% filter(aisle == "baking ingredients"), items_in_aisle)),2)`(`r round(sd(pull(top_products %>% filter(aisle == "baking ingredients"), items_in_aisle)),2)`), and _dog food care_ `r round(mean(pull(top_products %>% filter(aisle == "dog food care"), items_in_aisle)),2)`(`r round(sd(pull(top_products %>% filter(aisle == "dog food care"), items_in_aisle)),2)`).


### Day/Time orders occur

```{r}
# Part D:
#Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

apple_icecream = instacart %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  filter(product_name %in% c("Pink Lady Apple", "Coffee Ice Cream")) %>% 
  arrange(product_name, order_dow, order_hour_of_day) %>% 
  mutate(
    product_name = as.factor(product_name),
    order_dow = as.factor(order_dow), 
    order_dow = factor(order_dow, levels = c("0", "1", "2", "3", "4", "5", "6"), labels = c("0" = "Sunday", "1" = "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  ) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour_order = mean(order_hour_of_day)) %>% 
  mutate(
    time_mean_order = paste(floor(mean_hour_order), round((mean_hour_order - floor(mean_hour_order))*60), sep = ":")
  )

apple_icecream %>%
  select(-mean_hour_order) %>% 
  pivot_wider(
    names_from = product_name,
    values_from = time_mean_order 
  ) %>% 
  kableExtra::kable()


```

Overall, _Coffee Ice Cream_ was ordered later in the day than _Pink Lady Apple_. On average it is ordered latest on `r pull(apple_icecream%>% filter(product_name == "Coffee Ice Cream"), order_dow)[pull(apple_icecream %>% filter(product_name == "Coffee Ice Cream"), mean_hour_order) == max(pull(apple_icecream%>% filter(product_name == "Coffee Ice Cream"),mean_hour_order))]`s at `r pull(apple_icecream%>% filter(product_name == "Coffee Ice Cream"), time_mean_order)[pull(apple_icecream %>% filter(product_name == "Coffee Ice Cream"), mean_hour_order) == max(pull(apple_icecream%>% filter(product_name == "Coffee Ice Cream"), mean_hour_order))]`. This may be a high point for _Coffee Ice Cream_ as people are feeling sluggish mid-afternoon time at work. Whereas _Pink Lady Apple_ are ordered earlest on `r pull(apple_icecream%>% filter(product_name == "Pink Lady Apple"), order_dow)[pull(apple_icecream %>% filter(product_name == "Pink Lady Apple"), mean_hour_order) == min(pull(apple_icecream %>% filter(product_name == "Pink Lady Apple"),mean_hour_order))]`s at `r pull(apple_icecream%>% filter(product_name == "Pink Lady Apple"), time_mean_order)[pull(apple_icecream %>% filter(product_name == "Pink Lady Apple"), mean_hour_order) == min(pull(apple_icecream%>% filter(product_name == "Pink Lady Apple"), mean_hour_order))]`0 in the morning.  



# Problem 2
### Examining BRFSS data

```{r}

data("BRFSS")  

brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(question = as.factor(question),
         response = as.factor(response),
         response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered = T),
         question = as.factor(question)
  ) %>% 
  rename(
    health_rating = response,
    percent = data_value,
    state = locationabbr,
    county = locationdesc,
  ) %>% 
  dplyr::select(-c("question", "topic", "class", "topic_id", "location_id", "question_id", "class_id", "data_source", "data_value_type", "data_value_unit", "data_value_footnote", "data_value_footnote_symbol"))


# In 2002, which states were observed at 7 or more locations? What about in 2010?

# Part A
loca_df = 
  brfss_df %>% 
  distinct(state, county, year) %>% 
  arrange(year, state, county) %>% 
  group_by(state, year) %>% 
  add_count(state, name = "num_locations") %>% 
  distinct(year, state, num_locations) %>% 
  filter(num_locations >= 7)



```

In 2002, `r loca_df %>% filter(year == 2002) %>%  nrow()` states were observed at 7 or more locations (`r pull(loca_df %>% filter(year == 2002), state)`). In 2010, `r loca_df %>% filter(year == 2010) %>%  nrow()` states were observed at 7 or more locations (`r pull(loca_df %>% filter(year == 2010), state)`).



```{r}

# Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).


#Part B
ex_df = 
  brfss_df %>% 
  filter(health_rating == "Excellent") %>% 
  group_by(state, year) %>% 
  mutate(
    data_avg = mean(percent, na.rm = TRUE)
  ) %>% 
  distinct(state, year, data_avg)

ex_df %>% ggplot(aes(x = year, y = data_avg, color = state)) +
  geom_line(aes(group = state)) +
  labs(title = "Average excellent rating across time, by state",
       x = "Year",
       y = "Across state average percent excellent rating") +
  theme(legend.position = "right")

```

The state with the highest average excellent rating is `r x = pull(ex_df %>% group_by(state) %>% mutate(ex_avg = mean(data_avg)), state)[pull(ex_df %>% group_by(state) %>% mutate(ex_avg = mean(data_avg)) , ex_avg) == max(pull(ex_df %>% group_by(state) %>% mutate(ex_avg = mean(data_avg)), ex_avg))]; x[1]`, and the state with the lowest is `r y = pull(ex_df %>% group_by(state) %>% mutate(ex_avg = mean(data_avg)), state)[pull(ex_df %>% group_by(state) %>% mutate(ex_avg = mean(data_avg)) , ex_avg) == min(pull(ex_df %>% group_by(state) %>% mutate(ex_avg = mean(data_avg)), ex_avg))]; y[1]`.





```{r}

#Part C

# Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

ny_val = 
  brfss_df %>% 
  filter(state == "NY" & year %in% c(2006, 2010)) %>% 
  mutate(
    county = str_remove(county, "NY - ")
  ) 

ny_val %>% 
  ggplot(aes(x = health_rating, y = percent, color = county)) +
  geom_point() + 
  facet_grid(. ~ year) +
  theme_bw()  +
  labs(title = "Restaurant Health Ratings in NY Counties",
       x = "Health Rating",
       y = "Percent of Restaurants with Health Rating") +
  theme(legend.position = "bottom")




```


In 2006 the county with the highest Excellent rating was `r pull(ny_val %>% filter(year == 2006 & health_rating == "Excellent"), county)[pull(ny_val %>% filter(year == 2006 & health_rating == "Excellent"), percent) == max(pull(ny_val %>% filter(year == 2006 & health_rating == "Excellent"), percent))]` and the one with the highest Poor rating was `r pull(ny_val %>% filter(year == 2006 & health_rating == "Poor"), county)[pull(ny_val %>% filter(year == 2006 & health_rating == "Poor"), percent) == max(pull(ny_val %>% filter(year == 2006 & health_rating == "Poor"), percent))]`. In 2010 the county with the highest Excellent rating was `r pull(ny_val %>% filter(year == 2010 & health_rating == "Excellent"), county)[pull(ny_val %>% filter(year == 2010 & health_rating == "Excellent"), percent) == max(pull(ny_val %>% filter(year == 2010 & health_rating == "Excellent"), percent))]` and the one with the highest Poor rating was `r pull(ny_val %>% filter(year == 2010 & health_rating == "Poor"), county)[pull(ny_val %>% filter(year == 2010 & health_rating == "Poor"), percent) == max(pull(ny_val %>% filter(year == 2010 & health_rating == "Poor"), percent))]`. 


# Problem 3

```{r message = FALSE}
# read in data

# Part A - wrangle data
accel_df = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  rename(day_of_week = day) %>% 
  mutate(
    day_of_week = as.factor(day_of_week),
    day_of_week = ordered(day_of_week, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    weekend = ifelse(day_of_week %in% c("Saturday", "Sunday"), "weekend", "weekday"),
    weekend = as.factor(weekend)
  ) %>% 
  arrange(week, day_of_week) %>% 
  mutate(
    order_id = sequence(n())
  ) %>% 
  select(order_id, day_id, week, day_of_week, weekend, everything())


```

The accelerometer activity has `r nrow(accel_df)` rows with an observation for each day over `r max(pull(accel_df, week))` weeks, and `r ncol(accel_df)` variables, 1440 of which represent the level of activity for every minute of the observation day.

```{r}
# Part B - aggregate across activity

wide_accel = 
  accel_df %>% 
  mutate(
    act_sum = select(., activity_1:activity_1440) 
    %>% apply(1, sum, na.rm = TRUE),
  ) %>% 
  select(act_sum, week, day_of_week) 


wide_accel %>% 
  pivot_wider(
    names_from = day_of_week,
    values_from = act_sum
  )  %>% 
  kableExtra::kable(format.args = list(big.mark = ","), digits = 0) 


```

The man was most active on average on `r x = pull(wide_accel %>% group_by(day_of_week) %>%  mutate(sum_day = sum(act_sum)), day_of_week)[pull(wide_accel %>% group_by(day_of_week) %>%  mutate(sum_day = sum(act_sum)), sum_day) == max(pull(wide_accel %>% group_by(day_of_week) %>%  mutate(sum_day = sum(act_sum)), sum_day))]; x[1]` and least active on `r x = pull(wide_accel %>% group_by(day_of_week) %>%  mutate(sum_day = sum(act_sum)), day_of_week)[pull(wide_accel %>% group_by(day_of_week) %>%  mutate(sum_day = sum(act_sum)), sum_day) == min(pull(wide_accel %>% group_by(day_of_week) %>%  mutate(sum_day = sum(act_sum)), sum_day))]; x[1]`. His most active week was week `r x = pull(wide_accel %>% group_by(week) %>%  mutate(sum_day = sum(act_sum)), week)[pull(wide_accel %>% group_by(week) %>%  mutate(sum_day = sum(act_sum)), sum_day) == max(pull(wide_accel %>% group_by(week) %>%  mutate(sum_day = sum(act_sum)), sum_day))]; x[1]`, and least active was week `r x = pull(wide_accel %>% group_by(week) %>%  mutate(sum_day = sum(act_sum)), week)[pull(wide_accel %>% group_by(week) %>%  mutate(sum_day = sum(act_sum)), sum_day) == min(pull(wide_accel %>% group_by(week) %>%  mutate(sum_day = sum(act_sum)), sum_day))]; x[1]`.



```{r}
# Part C - figure across day

long_accel = accel_df %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "min_of_day",
    names_prefix = "activity_",
    values_to = "activity_per_min"
  ) %>% 
  mutate(
    min_of_day = as.integer(min_of_day)
  )


long_accel %>% 
  #filter(week == 1) %>% 
  ggplot(aes(x = min_of_day, y = activity_per_min, color = day_of_week)) +
  geom_line(aes(group = day_id), alpha = 0.7) +
  scale_x_continuous(breaks = seq(0, 1400, 200)) +
  scale_color_hue("Day of the week")  +
  labs(title = "Daily Accelerometer Activity Over 5 Weeks for 63 Year-old Male",
       x = "Minute of the Day",
       y = "Accelerometer Activity Level")

```


Based on the figure above, in general, there is higher latenight activity on Fridays, some late afternoon activity on saturdays, and higher daytime activity on Sundays. There also seems to be some early morning activity Sunday mornings, which would most likely mean continuation of activity from Saturday night.
